---
title: "Clustering Guide"
date: 2024-06-30
permalink: /posts/2023/03/clustering-guide/
tags:
  - machine learning
  - clustering
  - guide
---

Complete Guide to Clustering Algorithms

In machine learning, uncovering hidden patterns in data is the essential task of a machine learning engineer. Clustering algorithms, which group similar data points based on their characteristics, are key tools to have in our tool box in order to accomplish this.
First let's define clustering in the context of machine learning. Clustering is the task of grouping similar data points based on their characteristics. Clustering algorithms are unsupervised learning techniques, meaning that unlike supervised learning algorithms we do not have any prior known classes or labels that we are predicting. Instead these algorithms are intended to identify underlying data structures by assigning similar instances to a cluster.
There are many useful algorithms that fall under the umbrella of clustering each with their own pros and cons. Like many machine learning modeling decisions we are faced with considerations from two fronts. Optimal modeling results, selecting the model that is best able to find patterns in our data for the most useful predictions, and practical concerns, such as compute, time, and our dataset. Here I introduce many common models, their pros and cons, and what types of problem requirements should spring them to the front of your mind as a potential solution.

KMeans -
Definition
KMeans is the simplest and most commonly used algorithm for clustering. KMeans attempts to segment a dataset into a prespecified number K of distinct non-overlapping clusters. The algorithm starts by initializing a random K number of cluster centroids. The algorithm then alternates between assigning all data points to a cluster and choosing a new cluster at the center of the newly formed clusters. It then iterates through this process until the centroids converge or a maximum number of iterations is reached (sklearns default is 300).
Pros
Simple
Computationally efficient
Scales
Cons
Sensitive to unlucky centroid initialization

Requires a pre-specified number of clusters

Unable to identify non-spherical clusters

When to use
KMeans should typically be the first clustering algorithm that comes to your mind when a problem requiring clustering is presented. Being simple and efficient makes it easy to quickly run both in preprocessing for any modeling task as a way to better understand your data and as a baseline score prior to running more complex algorithms if you then deem them necessary. My general advice for clustering would be to run a KMeans as a baseline for your clustering task unless you have some prior knowledge going in that the data will for a some complex non-spherical shape.

Agglomerative Clustering -
Definition
Agglomerative clustering is the process of allowing each data point to be its own cluster then with each iteration combining the two most similar clusters given some linkage criteria. The most commonly used linkage criteria for agglomerative clustering is called ward which picks the two clusters to merge such that the variance within all clusters increases the least.
Pros
Does not require a pre-specified number of clusters

Plotting as a dendrogram can lead to further insights at many different levels in the data rather than potentially just one like most other algorithms
Cons
Computationally inefficient on large datasets

Difficult to interpret results for large datasets

When to use

Agglomerative clustering is best used on smaller datasets. The nature of iteratively clustering one cluster at a time starting from individual data points causes the computational load to quickly balloon as the size of the dataset grows. That being said, this algorithm can be most useful in two scenarios, one for exploring a smaller dataset similarly to KMeans. Or two, modeling on a small dataset that may have interesting clustering on both a more detailed level as well as a general level.

DBSCAN (Density-Based Spatial Clustering for Applications with Noise) -
Definition

DBSCAN is a density based approach to clustering. That means that rather than optimizing for a center of a cluster and calculating distance to that point DBSCAN is instead finding densely compact data points. It is able to accomplish this by finding the number of points in our data that are within a small distance ε of our initial point. ε is defined as a parameter we select for the model. If there are at least min_samples, another parameter we select for our model, then that data point is considered a core instance of a cluster. All core instances and their neighbors are said to be in a cluster and all that are not are denoted as anomalies.
Pros

Computationally efficient

Does not require a pre-specified number of clusters

Capable of capturing non-spherical clusters

Robust to outliers

Cons
Unable to capture clusters of varying density

Fails in high dimensionality where density becomes less meaningful
When to use
DBSCAN is a very useful tool to have because it is very effective in ways that more common clustering algorithms fail. The primary example of this is how effective DBSCAN is identifying clusters with arbitrary shapes. DBSCAN’s algorithm is tailored towards finding regions of dense data points rather than cluster centroids so it is able to find more complex patterns than algorithms like KMeans. This along with DBSCAN’s proficiency in handling outliers in data are the main reasons for picking it over other options. So with these things in mind we can reason that DBSCAN should be our choice in two main scenarios. One, we have found that the data is forming some arbitrary shape that we want to capture. This intuition usually comes after initial data exploration such as plotting finds the data to be in complex structures or if we try a preliminary centroid clustering algorithm such as KMeans and it is unable to identify all of the complexity of our dataset. Or two, we believe that the dataset has a significant number of outliers that would otherwise negatively impact some of the other models that we have discussed that are more susceptible to noise impacting clustering results.

    Clustering can be useful in a multitude of situations. Let's group (or cluster rather) them into a few potential instances you may find them useful.

Data analysis
Prior to any modeling it is critical to get a feel for your dataset. When first exploring a dataset we can use clustering as a way to get more familiar with our data. A common tactic to achieve this is to implement PCA to reduce the dimensions of your dataset to two followed by implementing a simple clustering algorithm such as Kmeans and plotting the results. This is a quick way to find some underlying trends in our data as we can then reason about what underlying mechanism may be at play given this new visualization.

Dimensionality Reduction
Clustering can also be used as another form of dimensionality reduction. After fitting a clustering algorithm to our dataset we can measure each data point's affinity to each of the created clusters using a distance measurement. For example, we could again use a clustering algorithm such as KMeans (or any other centroid clustering algorithm) to return k number of centroid clusters. We then would calculate the distance of each data point to each centroid to get a vector of k-dimensions containing the distances of each point to each cluster centroid. We can then use this vector as in place of our original feature vector while still preserving enough information for further modeling.

Recommendation Systems
Clustering is also a key component of recommendation algorithms. Many companies such as Netflix, Amazon, and Facebook are using combinations of machine learning algorithms including clustering algorithms to recommend content to you. So how do they do it? Say we have a dataset containing user preference data. In this example we will say user netflix history. In this dataset we have a bunch of features that we can associate to a user's preference. This would look something like duration of watch, user reviews, searching data, ect. Of course not all of our users have watched every show on netflix or given them all ratings. A possible way to predict how they would rate unwatched shows would be by creating a matrix of a user preference metric for each show (we will use rating 1-5 ) R and of all the users on the platform U [R x U]. Importantly we can then fill any null values (shows users haven’t rated) as zeroes. Now we can fit a Gaussian Mixture Model to our matrix. Using the means and covariances of the clusters we produce from this model we can calculate the log-likelihood of each user belonging to each gaussian cluster. With the log-likelihood we can then calculate the probabilities of each user belonging to a cluster (by converting our log-likelihood into a soft assignment for each user). Then all we have to do is compute predicted ratings for unrated shows for each user using the weighted sum of the Gaussian cluster means. With our predicted ratings now we can prioritize shows at the top of their feed that our model predicts they would rate highest!

Anomaly Detection
The last commonly found use case we will discuss is using clustering algorithms for anomaly detection. Anomaly detection can prove useful in two primary ways, pre-model or the model itself, I will lay both out here. First, we can use anomaly detection as a part of our preprocessing steps as we explore the data. It is often useful for us to find anomalies in our dataset so we can take proper action prior to allowing them to throw off our modeling. This can be done during our data analysis step after our PCA and KMeans tactic is completed. We can quickly spot anomalies by looking for data points on our KMeans plot that have attracted an entire cluster to themselves or maybe there are a few of these instances. As far as what to do from there we will have to use our situational awareness to decide whether we would want to drop the outliers, do some normalization technique or similar transformation, or keep it if it is useful. Secondly, we may want to do a more advanced anomaly detection method as the means for our model. Let’s say for instance that rather than as a simple preprocessing step we want to create a model specifically for anomaly detection. A very common request particularly in the manufacturing industry is to use anomaly detection for machine sensory data to detect potential malfunctions as they happen (or optimally before). There are many statistical approaches that could be taken to accomplish this task and it truly depends on the needs of the company in each situation. That being said, we can imagine a manufacturing company with a very complex machine. The machine has dozens of sensors that are detecting many things at once: temperature, pressure, humidity, vibration, speed of conveyor belts, etc. All of these sensors form a complex relationship that we may not have a lot of prior intuition about what types of changes will cause issues and what won’t. If we don’t have a lot of historical data for a supervised learning approach we may need to use a clustering algorithm such as DBSCAN (Density-Based Spatial Clustering for Applications with Noise). This algorithm is intriguing in this case because it is well suited for dealing with high dimensional data for the following reasons. We do not need to specify the number of clusters prior to modeling, It naturally identifies noise (potential anomalies in our case), and handles arbitrary shapes which may be necessary in this circumstance as the many sensors may form complex structures that fall into normal operation. The process for implementing such an algorithm would look like this. First, normalize the sensor data so that each feature contributes equally to DBSCAN’s distance calculation. Next we will need to determine appropriate values for ε and min samples. ε is the small distance that a particular data point needs to have at least min samples within in order for our algorithm to make it a core instance. To do this we can run our model through several iterations of ε and min_sample combinations and compute the silhouette scores of each model. The combination that yields the best score is a great starting point for our model variables. This leaves us with only two tasks, running the model and developing a useful metric from it to allow the machine operators to understand when the machine is producing more anomalies. This metric could look something like a percentage of instances that the machine produces that our model tags as an anomaly. This percentage could be sampled on a time interval or if necessary as a rolling calculation for each instance. To give the machine operators an obvious intervention point you could even develop a trigger threshold where if the percentage of anomalies exceeds the threshold an inspection of the machine is recommended. This threshold could be set using a statistical model such as SPC (statistical process control) to determine rules for when the trigger should be set off given historical data of the machine.
